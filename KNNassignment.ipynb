{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45db805-5235-4233-b9db-0e0cb7fa4de4",
   "metadata": {},
   "source": [
    "Ans . 1 \n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple and widely used classification and regression technique in machine learning. It is primarily used for solving classification problems, but it can also be adapted for regression tasks.  It calculate the distance btw the new data point and k nearest point of the traning data nd on the basis of it. tells us about the new data point i,e the groups of point which are more close to new data point become the category of the new point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb54141-1464-4e13-b769-e50ab128a6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63b5104d-431c-4080-9a93-056630d894b9",
   "metadata": {},
   "source": [
    "Ans. 2\n",
    "\n",
    "we calculate the distance btw the new data point and the training data points (orignal points), and the points which are more closer to the test data are considered , we also use Hyperparameter tuning for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3a992-acbf-4a11-ae11-81f5a922aac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69e746ef-948b-40f1-b4d4-e10b6a0e645e",
   "metadata": {},
   "source": [
    "Ans. 3 \n",
    "\n",
    "The KNN classifier is used for classification tasks. It's used when we want to predict the categorical class label of a new data point based on the classes of its K nearest neighbors. In KNN classification, the majority class among the K neighbors is assigned to the new data point. The class with the highest count among the neighbors is considered the predicted class for the new point. \n",
    "\n",
    "\n",
    "The KNN regressor is used for regression tasks. It's employed when we want to predict a continuous numerical value for a new data point based on the values of its K nearest neighbors. In KNN regression, the predicted value for the new data point is often calculated as the average (or weighted average) of the target values of its K neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f3938-4170-406c-8578-978b24ba25a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b895655-e2ed-4e55-8d43-2eb035517dce",
   "metadata": {},
   "source": [
    "Ans. 4\n",
    "\n",
    "The performance of a K-Nearest Neighbors (KNN) model can be evaluated using various metrics and techniques that are commonly used for classification and regression tasks are accuracy score , classfication metrics , confusion metrix , recall and f1 score in case of classification and in case of regression we can use r2 score, mean squared error , mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d9b75-cebe-46cc-a7c5-2590734d4f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f615428-47ae-4cc2-8391-a9efcfb0f723",
   "metadata": {},
   "source": [
    "Ans. 5 \n",
    "\n",
    "The \"curse of dimensionality\" is a term used to describe the challenges and issues that arise when working with high-dimensional data in machine learning and data analysis. It particularly affects algorithms like K-Nearest Neighbors (KNN) as the number of dimensions (features) in the dataset increases. The curse of dimensionality can have a significant impact on the performance and efficiency of KNN and other distance-based algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ffd60-462a-47d3-a117-1500c5cc1113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61c0be2f-9f30-4fb6-aa02-2f497bb8e2b7",
   "metadata": {},
   "source": [
    "Ans. 6\n",
    "\n",
    "Replace missing values with global statistics such as mean, median, or mode of the entire feature. This can be a simple and quick way to fill in missing values, but it might not capture specific relationships within neighborhoods of data points.\n",
    "\n",
    "we can also Replace missing values with the mean, median, or mode of the feature within the specific class or cluster of the instance. This can be more informative than global statistics as it takes into account the characteristics of similar instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd43e2-b7bd-41e4-91aa-c0c0673906f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e4dfe51-65ea-4a5c-9629-0c72a87e3c68",
   "metadata": {},
   "source": [
    "Ans. 7\n",
    "\n",
    "The performance of K-Nearest Neighbors (KNN) classifier and regressor can vary depending on the nature of the problem, the characteristics of the data, and the specific goals of the analysis. \n",
    "\n",
    "- KNN classifier is used for classification tasks, where the goal is to assign data points to predefined classes or categories. The output of a KNN classifier is a categorical class label. Common evaluation metrics include accuracy, precision, recall, F1-score, and ROC curve. KNN classification works well with both nominal and ordinal categorical data. KNN classification is suitable for problems such as image recognition, text categorization, spam detection, and medical diagnosis.\n",
    "\n",
    "- KNN regressor is used for regression tasks, where the goal is to predict a continuous numerical value based on input features. The output of a KNN regressor is a numerical value. Common evaluation metrics include mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and R-squared. KNN regression is well-suited for problems involving continuous or interval data. KNN regression is appropriate for tasks such as predicting housing prices, stock prices, temperature forecasting, and demand prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d11101-a5d8-439a-b486-aba717755def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "383f56dc-47cd-49f5-b8e3-9699225f2f82",
   "metadata": {},
   "source": [
    "Ans. 8 \n",
    "\n",
    "- Strength of KNN : \n",
    "KNN is easy to understand and implement. The basic idea of finding neighbors and making predictions based on their attributes is intuitive and requires minimal mathematical background. KNN is a non-parametric algorithm, meaning it doesn't make strong assumptions about the underlying data distribution. This makes it more flexible in capturing complex relationships.\n",
    "\n",
    "- weakness of KNN :\n",
    "The time complexity of KNN increases as the dataset grows, as it requires calculating distances between the query point and all training data points. For large datasets, this can make KNN slow and resource-intensive. KNN can be sensitive to noisy data and outliers, as they can significantly affect the calculation of distances and influence the predictions.\n",
    "\n",
    "- ways to address the weakness : \n",
    "Choose appropriate distance metrics based on the nature of the data. For example, use cosine similarity for text data or specialized distance metrics for high-dimensional spaces. Normalize or scale the features to ensure that all dimensions contribute equally to distance calculations. Use outlier detection techniques to identify and handle noisy data points. This might involve removing them or imputing more reasonable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cff16-3c03-4595-8e82-b5729eb02e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc13756-9cec-4f35-82cf-db2ed90af56f",
   "metadata": {},
   "source": [
    "Ans. 9 \n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in a Euclidean space. It's calculated as the square root of the sum of the squared differences between corresponding coordinates of the two points. Mathematically, for two points (x1, y1) and (x2, y2) in a 2D space:\n",
    "\n",
    "Euclidean Distance = âˆš((x2 - x1)^2 + (y2 - y1)^2) \n",
    "\n",
    "\n",
    "Manhattan distance (also known as taxicab distance or city block distance) measures the distance between two points by summing the absolute differences between their coordinates. It's like calculating the distance a taxicab would travel to get from one point to another in a grid-like city layout. Mathematically, for two points (x1, y1) and (x2, y2) in a 2D space:\n",
    "\n",
    "Manhattan Distance = |x2 - x1| + |y2 - y1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e305c38-4c1f-4877-8320-4f14cf9db517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e413384-a607-4cb5-b98a-81934f7b7dcd",
   "metadata": {},
   "source": [
    "Ans. 10\n",
    "\n",
    "Feature scaling plays a crucial role in K-Nearest Neighbors (KNN) algorithm to ensure that all features contribute equally to the distance calculations between data points. The distances between data points are a fundamental component of KNN, as they determine which points are considered nearest neighbors. If the features have different scales or ranges, certain features might dominate the distance calculations simply due to their larger magnitude, leading to biased results.\n",
    "\n",
    "Feature scaling helps normalize the features, ensuring that they are on similar scales. This prevents features with larger values from overwhelming those with smaller values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d7659-f73a-47d1-b529-286dd926316e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
